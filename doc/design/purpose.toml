[REQ-collection_to_csv]
partof = 'REQ-purpose'
text = '''
The one main motivation for providing an option to write the collection of data
to a .csv-file is to make it easy to import to Excel and LibreOffice.

It is further important that the choice of columns make it easy to create
different kind of charts for presentation.
'''

[REQ-data_gather_frequency]
partof = 'REQ-purpose'
text = '''
The gather of data should be frequently ran.

I think it is important that different suites of tests can be ran with
different frequencies. Heavy probably less often than those that are easy to
test.

It is PROBABLY important that those tests that test a shared resource do NOT
run in parallell from many servers.
'''

[REQ-gather_metric]
partof = 'REQ-purpose'
text = '''
Metrics are gathered by running a bundle of plugins that _send_ back data to
the plugin runner.
'''

[REQ-infrastructure]
partof = 'REQ-purpose'
text = '''
Metrics of the infrastructure.

The metrics should focus on:
 * performance
 * resource usage
'''

[REQ-infrastructure-resource_usage]
text = '''
This group of metrics shall capture the health of the infrastructure.

The infrastructure is the hardware and network.

It is important that the measurment is both in absolut numbers and in
percentage.

This kind of metric are:
 * memory usage
 * cpu usage
 * users in the system that uses most resources
 * programs that uses the most resources
 * I/O. such as disc, network
 * unix load average
'''

[REQ-parallell_test_host_execution]
partof = 'REQ-purpose'
text = '''
Running the tests on remote hosts should be done in parallell to keep down the
total runtime of the test suite.
'''

[REQ-plugin]
partof = 'REQ-purpose'
text = '''
MetricFactory shall provide an API for registering plugins that perform metric
gathering.

These plugins are intended to be written by the user.
'''

[REQ-purpose]
text = '''
The purpose of this project is to provide a framework for metrics.

The main categories are:
 * infrastructure
 * workflow

It is important that the data gatherer and data analyzer is separated.
The data should be stored in an open format that is easy to analyse.
Easy to use by other programs.
'''

[REQ-remote_test_host_execution]
partof = 'REQ-gather_metric'
text = '''
ditto
'''

[REQ-workflow]
partof = 'REQ-purpose'
text = '''
Metrics of workflow.

The metrics should focus on:
 * completed runs of the defined workflow (throughput)
 * the complete time for finishing a workflow (delay)
 * time spent for workunits

A workunit is the smallest part that is measured in a workflow.
'''

[SPC-collection_serialiser]
partof = 'SPC-remote_test_host_execution'
text = '''
Serialization of the data in a collection shall support full and limited
serialization.

Full serialization is all the data that is in the container.
This uses a custom file format.

Limited serialization is to a text file containing the textual subset of the
collection that are supported by StatsD.
This is Counter, Gauge, Set and Timer.
'''

[SPC-collection_serialiser-test_host_name]
text = '''
To differentiate the measurments from each other the collected metrics shall
have a hostname tag.
'''

[SPC-collection_to_csv]
text = '''

The columns shall be:
 - index. A monotonic increasing number starting from 1.
   This is intended to make it easy to create a chart by using this index for the X axis.
 - description. Describes the test.
   It is important that it is compatible with the backends.
 - host. Name of the test host the test was ran on.
   Makes it possible to do a deeper analysis of the gathered metrics.
 - date.
   It is easier to use the date and time in e.g Excel if they are separated in
   two columns.
 - time.
 - value. The value of a gauge.
 - change. The change of a counter.
 - min, max, sum, mean. The times from a timer.
'''

[SPC-collector_deserialize]
partof = 'SPC-remote_test_host_execution'
text = '''
The inverse of [[SPC-collection_serialiser]]
'''

[SPC-dataformat]
partof = 'REQ-purpose'
text = '''
The user wants an easy format to import to Excel.
The metric factory itself needs a nice format for serialiation/deserialise of
test results from a remote host.

The metric factor shall:
 - support writing the result to a CSV file.
 - support writing the result to a data file that store each value as a StatsD
   compatible value.

TODO maybe move this req.
'''

[SPC-infrastructure_io]
partof = 'REQ-infrastructure-resource_usage'
text = '''
Gather statistics about I/O performance.
'''

[SPC-infrastructure_io-listdir]
text = '''
Gather statistics for listing a directory.

Maybe this should test both the cold and warm listing?
How can it be assured it is cold?
'''

[SPC-infrastructure_io-smallfile_perf]
text = '''
Gather statistics of the filesystem performance by creating and remove 10k files.

The intention is to simulate the part of a compilation process that interact
with the filesystem.

The data that is written should be random as to avoid deduplication of blocks
that can occur on a ZFS filesystem.
'''

[SPC-infrastructure_memory]
partof = 'REQ-infrastructure-resource_usage'
text = '''
The top 5 processes that uses memory (RSS) shall be measured.
The total available physical memory shall be measured.

This will help to find the worst offenders.

Note: Should the total used memory be measured?
'''

[SPC-infrastructure_program]
partof = 'REQ-infrastructure-resource_usage'
text = '''
Gather statistics for the top 100 programs that are currently running on the server.

Consider if the usage should be of the totally available.
This one is a bit hard because the value may need to be normalized in some way to account for differences between servers.
What I'm trying to say is that an absolute number would be very misleading.

Statistics:
 * memory used
 * cpu used
 * user
 * server
'''

[SPC-remote_test_host_execution]
text = '''
To keep the design simple to begin with the test results are stored in a local
file that are retrieve by the master node.

Pseud-code:
# one time
mktmp results_XXXXX

# loop over test hosts
ssh host mkdir /tmp/<random>
scp metric_factory host:/tmp/<random>
ssh host:/tmp/<random>/metric_factory --run
(remote hosts serialise the collection to a data file)
scp host:/tmp/<random>/result.dat results_XXXXX

# summarise the results
deserialise the collected datafiles to a master collection
'''

[SPC-execution_mode_for_plugins]
partof = 'REQ-parallell_test_host_execution'
text = '''
The program shall support plugins to be tagged with an execution mode that
determine *how* it is executed.

Execution modes:
 - allHost. Shall be executed on all test hosts.
 - single. Shall only be executed once.

allHost shall be the default execution mode.
'''
